{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── README\r\n",
      "├── Tutorial-Copy1.ipynb\r\n",
      "├── Tutorial.ipynb\r\n",
      "├── Untitled.ipynb\r\n",
      "├── imdb.vocab\r\n",
      "├── imdbEr.txt\r\n",
      "├── \u001b[01;34mtest\u001b[00m\r\n",
      "│   ├── labeledBow.feat\r\n",
      "│   ├── \u001b[01;34mneg\u001b[00m\r\n",
      "│   ├── \u001b[01;34mpos\u001b[00m\r\n",
      "│   ├── urls_neg.txt\r\n",
      "│   └── urls_pos.txt\r\n",
      "└── \u001b[01;34mtrain\u001b[00m\r\n",
      "    ├── labeledBow.feat\r\n",
      "    ├── \u001b[01;34mneg\u001b[00m\r\n",
      "    ├── \u001b[01;34mpos\u001b[00m\r\n",
      "    ├── \u001b[01;34munsup\u001b[00m\r\n",
      "    ├── unsupBow.feat\r\n",
      "    ├── urls_neg.txt\r\n",
      "    ├── urls_pos.txt\r\n",
      "    └── urls_unsup.txt\r\n",
      "\r\n",
      "7 directories, 14 files\r\n"
     ]
    }
   ],
   "source": [
    "#Make sure your are inside the folder\n",
    "#-L: level of details\n",
    "#-C: color for folders\n",
    "!tree -L 2 -C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "review_train = load_files('train/',categories=['neg','pos'])\n",
    "type(review_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 (25000,)\n"
     ]
    }
   ],
   "source": [
    "X,y = review_train.data, review_train.target\n",
    "print(len(X), y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\"\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. Flawed but honest with a terrible honesty.\"\n"
     ]
    }
   ],
   "source": [
    "#Remove newline break <br />\n",
    "X = [txt.replace(b'<br />',b'') for txt in X]\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 1 1 0 0 1] [1 1 1 0 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#checking y: first 10 and last 10 elements of y\n",
    "print(y[:10],y[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLP, text processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vect = CountVectorizer()\n",
    "#vect = TfidfVectorizer(ngram_range=(1,2), min_df=3)\n",
    "\n",
    "vect.fit(X)\n",
    "x = vect.transform(X)\n",
    "type(x)\n",
    "#print('Text processing complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "75911\n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007']\n",
      "['était', 'état', 'étc', 'évery', 'êxtase', 'ís', 'ísnt', 'østbye', 'über', 'üvegtigris']\n",
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n",
      "[[ 93 300   1 ...,   1   4   1]]\n",
      "[[37955 39352 39354 ..., 47352  3375 67244]]\n",
      "[('was', 48198), ('that', 73274), ('this', 75965), ('in', 93955), ('it', 96436), ('is', 107313), ('to', 135706), ('of', 145851), ('and', 164109), ('the', 336633)]\n",
      "[('labourers', 1), ('lilienthal', 1), ('lilililililii', 1), ('lilleheie', 1), ('lillete', 1), ('lilley', 1), ('lillihamer', 1), ('liman', 1), ('likings', 1), ('limbless', 1)]\n"
     ]
    }
   ],
   "source": [
    "#How many words in my vocabulary\n",
    "#What are they?\n",
    "\n",
    "vocabulary = vect.get_feature_names()\n",
    "print(type(vocabulary))\n",
    "print(len(vocabulary))\n",
    "print(vocabulary[:10])\n",
    "print(vocabulary[-10:])\n",
    "\n",
    "#The most common / least common words\n",
    "import numpy as np\n",
    "words_total_counts = np.sum(x, axis=0)\n",
    "print(type(words_total_counts))\n",
    "print(words_total_counts)\n",
    "\n",
    "#np.sort vs np.argsort()\n",
    "idx_sorted_by_counts = np.argsort(words_total_counts)\n",
    "print(idx_sorted_by_counts)\n",
    "\n",
    "#list comprehension\n",
    "most_common_words  = [vocabulary[idx_sorted_by_counts[0,i]] for i in range(-10,0)]\n",
    "most_common_counts = [words_total_counts[0,idx_sorted_by_counts[0,i]] for i in range(-10,0)]\n",
    "\n",
    "least_common_words  = [vocabulary[idx_sorted_by_counts[0,i]] for i in range(10)]\n",
    "least_common_counts = [words_total_counts[0,idx_sorted_by_counts[0,i]] for i in range(10)]\n",
    "\n",
    "print(list(zip(most_common_words,most_common_counts)))\n",
    "print(list(zip(least_common_words,least_common_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use LogisticRegression to train the classifier\n",
    "# x, y\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(x,y) #training done!\n",
    "\n",
    "#predict\n",
    "review_test = load_files('test/',categories=['neg', 'pos'])\n",
    "X_test, y_test = review_test.data, review_test.target\n",
    "x_test = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99831999999999999"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training score\n",
    "lgr.score(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86704\n"
     ]
    }
   ],
   "source": [
    "#test score\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = lgr.predict(x_test)\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1}\n",
      "0.88844\n"
     ]
    }
   ],
   "source": [
    "#Regularization parameter C\n",
    "#reduce overfitting\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param = {'C': [0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param, cv=5, n_jobs=-1)\n",
    "grid.fit(x,y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87892\n"
     ]
    }
   ],
   "source": [
    "#Improved accuracy\n",
    "y_pred = grid.predict(x_test)\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use a different Text processing technique: Tfidf vectorization\n",
    "#vect = CountVectorizer()\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), min_df=3)\n",
    "\n",
    "vect.fit(X)\n",
    "x = vect.transform(X)\n",
    "type(x)\n",
    "\n",
    "x_test = vect.transform(X_test)\n",
    "\n",
    "param = {'C': [0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param, cv=5, n_jobs=-1)\n",
    "grid.fit(x,y)\n",
    "\n",
    "y_pred = grid.predict(x_test)\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "#Much improved accuracy with Tfidf!!! 0.907"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around and have some fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
